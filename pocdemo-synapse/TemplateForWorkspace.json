{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "pocdemo-synapse"
		},
		"pocdemo-synapse-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'pocdemo-synapse-WorkspaceDefaultSqlServer'"
		},
		"pocdemo-synapse-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://pochtmldemotoadls.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/pocdemo-synapse-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('pocdemo-synapse-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/pocdemo-synapse-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('pocdemo-synapse-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sas_to_parquet_using_python')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "cde1687e-54ac-48fe-94f8-9c7149f4b922"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/505b2bdc-ae47-42f6-8aec-7df3585952f9/resourceGroups/otpocdemo/providers/Microsoft.Synapse/workspaces/pocdemo-synapse/bigDataPools/sparkpool",
						"name": "sparkpool",
						"type": "Spark",
						"endpoint": "https://pocdemo-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"application/vnd.databricks.v1+cell": {
								"title": "",
								"showTitle": false,
								"nuid": "52fda352-2b36-4272-a3a9-a08b3acd6625"
							}
						},
						"source": [
							"#Install Packages\n",
							"%pip install sas7bdat\n",
							"%pip install --upgrade pip\n",
							"\n",
							"#Import Packages\n",
							"import pandas as pd\n",
							"import pyarrow as pa\n",
							"import pyarrow.parquet as pq\n",
							"from sas7bdat import SAS7BDAT\n",
							"\n",
							"# *****************************************************************************\n",
							"# Added to mount to the Azure Storage Account\n",
							"\n",
							"mount_name = '/mnt/nicoleblbstg'\n",
							"\n",
							"if not any(mount.mountPoint == mount_name for mount in dbutils.fs.mounts()):              \n",
							"    dbutils.fs.mount(\n",
							"        source = \"wasbs://nicoleblbstg@nicoleblbstg.blob.core.windows.net\",\n",
							"        mount_point = mount_name,\n",
							"        extra_configs = {\"fs.azure.account.key.nicoleblbstg.blob.core.windows.net\":\"IekjFA+sqW3MwykZV7B4S5/7dzbB600psa9QyWnfYZDMBCylvKQvXcdnR17S1Vo3wvpuUMsDOv1Dh009DMEjQw==\"}) \n",
							"# *****************************************************************************\n",
							"\n",
							"\n",
							"#read and write required Metadata columns\n",
							"with SAS7BDAT('/dbfs/mnt/nicoleblbstg/sample_dataset_contents.sas7bdat', skip_header=False) as metadata:\n",
							"  #Put Data into Pandas Dataframe\n",
							"  df_metadata = metadata.to_data_frame()\n",
							"  df_metadata.to_parquet('/dbfs/mnt/nicoleblbstg/sas-to-parquet-using-python/sample_dataset_contents.parquet',compression='SNAPPY')  \n",
							"\n",
							"def sas_to_parquet(filelist, source, destination):\n",
							"    rows = 0\n",
							"    for i, filename in enumerate(filelist):\n",
							"      #read data into Pandas Dataframe\n",
							"      with SAS7BDAT(source+filename+'.sas7bdat', skip_header=False) as reader:\n",
							"            #Put Data into Pandas Dataframe\n",
							"            df = reader.to_data_frame()\n",
							"            #Write to Parquet\n",
							"            df.to_parquet(destination+filename+'.parquet',compression='SNAPPY')  \n",
							"########################## Not necessary purely for viewing purposes ##########################\n",
							"            #Read Parquet to validate\n",
							"            df_parquet = pd.read_parquet(destination+filename+'.parquet')\n",
							"            #List attributes\n",
							"            df_parquet.info(verbose=True)\n",
							"            print(df_parquet)\n",
							"########################## End ##########################\n",
							"                      \n",
							"# SaS files to process into Parquet Files (Contains list of files, sourcedirectory, destinationdirectory)   \n",
							"sas_to_parquet(['sample_file_1','sample_file_2','sample_file_3','sample_file_4','sample_file_3_formats_data'], \n",
							"               '/dbfs/mnt/nicoleblbstg/','/dbfs/mnt/nicoleblbstg/sas-to-parquet-using-python/')\n",
							"\n",
							"        "
						],
						"outputs": [],
						"execution_count": 0
					},
					{
						"cell_type": "code",
						"metadata": {
							"application/vnd.databricks.v1+cell": {
								"title": "",
								"showTitle": false,
								"nuid": "079bc4d6-41ba-4811-a80a-4d9ee011ce43"
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": 0
					},
					{
						"cell_type": "code",
						"metadata": {
							"application/vnd.databricks.v1+cell": {
								"title": "",
								"showTitle": false,
								"nuid": "9ac6d9e6-933e-433c-98bb-f8fa82abf6be"
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": 0
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sas_to_parquet_using_spark')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"description": "thsi is import from databricks",
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "444d016f-8f68-445b-9bcc-b1cde9d7ec5b"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/505b2bdc-ae47-42f6-8aec-7df3585952f9/resourceGroups/otpocdemo/providers/Microsoft.Synapse/workspaces/pocdemo-synapse/bigDataPools/sparkpool",
						"name": "sparkpool",
						"type": "Spark",
						"endpoint": "https://pocdemo-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					}
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"application/vnd.databricks.v1+cell": {
								"title": "",
								"showTitle": false,
								"nuid": "5b5753e2-7a66-4f60-983a-2242a01ec6d6"
							}
						},
						"source": [
							"# *****************************************************************************\n",
							"# Added to mount to the Azure Storage Account\n",
							"\n",
							"mount_name = '/mnt/nicoleblbstg'\n",
							"\n",
							"if not any(mount.mountPoint == mount_name for mount in dbutils.fs.mounts()):              \n",
							"    dbutils.fs.mount(\n",
							"        source = \"wasbs://nicoleblbstg@nicoleblbstg.blob.core.windows.net\",\n",
							"        mount_point = mount_name,\n",
							"        extra_configs = {\"fs.azure.account.key.nicoleblbstg.blob.core.windows.net\":\"IekjFA+sqW3MwykZV7B4S5/7dzbB600psa9QyWnfYZDMBCylvKQvXcdnR17S1Vo3wvpuUMsDOv1Dh009DMEjQw==\"}) \n",
							"# *****************************************************************************\n",
							"\n",
							"# ***********************************Set Up ***********************************\n",
							"#Install packages, if not installed for first run remove the comments on sas7bdat and/or pandas\n",
							"%pip install sas7bdat\n",
							"%pip install --upgrade pip\n",
							"\n",
							"#import required packages\n",
							"#import os\n",
							"#try:\n",
							"#    import sas7bdat\n",
							"#    import pandas\n",
							"#    import fastparquet\n",
							"#    import findspark\n",
							"#    findspark.init()\n",
							"#except ImportError:\n",
							"#    print('try to install the packags first')\n",
							" \n",
							"#from sas7bdat import SAS7BDAT\n",
							"    \n",
							"# Import PySpark\n",
							"import pyspark\n",
							"from pyspark.sql import SparkSession\n",
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
							"import sas7bdat\n",
							"\n",
							"    \n",
							"if type(sqlContext) != pyspark.sql.context.HiveContext:\n",
							"    print('reset the Spark SQL context')\n",
							"    \n",
							"# *****************************************************************************\n",
							"\n",
							"\n",
							"\n",
							"#read and write required Metadata columns\n",
							"    df = spark.read.format(\"com.github.saurfang.sas.spark\").load(\"/mnt/nicoleblbstg/sample_dataset_contents.sas7bdat\", \n",
							"                                                                 forceLowercaseNames=True, inferLong=True)\n",
							"    df.write.mode('overwrite').parquet('/dbfs/mnt/nicoleblbstg/sas-to-parquet-using-spark/sample_dataset_contents')\n",
							"    display(df)\n",
							"\n",
							"def sas_to_parquet(filelist, source, destination):\n",
							"    rows = 0\n",
							"    for i, filename in enumerate(filelist):\n",
							"     #Read sas7bdat into Spark Data frame\n",
							"     spark_df = spark.read.format(\"com.github.saurfang.sas.spark\").load(source+filename+'.sas7bdat', \n",
							"                                                                        forceLowercaseNames=True, inferLong=True)\n",
							"     #Write to Parquet File\n",
							"     spark_df.write.mode('overwrite').parquet(destination+filename)\n",
							"###################### not needed for display purposes ######################\n",
							"     data = spark.read.parquet(destination+filename)\n",
							"     display(data)\n",
							"     print(\"Schema: {}\".format(data.schema))\n",
							"###################### not needed for display purposes ######################\n",
							"sas_to_parquet(['sample_file_1','sample_file_3','sample_file_4','sample_file_2'], \n",
							"               '/mnt/nicoleblbstg/', '/mnt/nicoleblbstg/sas-to-parquet-using-spark/')\n",
							"#sas_to_parquet(['sample_file_1'], '/dbfs/mnt/nicoleblbstg/', '/mnt/nicoleblbstg/sas-to-parquet-using-spark/')    \n",
							"        "
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"application/vnd.databricks.v1+cell": {
								"title": "",
								"showTitle": false,
								"nuid": "f8a2331d-8d83-4302-9fb3-6e6ca78901f4"
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/test env')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "8aebc98d-eceb-4998-9851-79edce8e7669"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/505b2bdc-ae47-42f6-8aec-7df3585952f9/resourceGroups/otpocdemo/providers/Microsoft.Synapse/workspaces/pocdemo-synapse/bigDataPools/sparkpool",
						"name": "sparkpool",
						"type": "Spark",
						"endpoint": "https://pocdemo-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28,
						"automaticScaleJobs": true
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# ***********************************Set Up ***********************************\r\n",
							"# Import PySpark\r\n",
							"import pyspark\r\n",
							"from pyspark.sql import SparkSession\r\n",
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType\r\n",
							"import sas7bdat\r\n",
							"    \r\n",
							"if type(sqlContext) != pyspark.sql.context.HiveContext:\r\n",
							"    print('reset the Spark SQL context')\r\n",
							"df = spark.read.format(\"com.github.saurfang.sas.spark\").load(\"/mnt/nicoleblbstg/sample_dataset_contents.sas7bdat\", \r\n",
							"                                                                 forceLowercaseNames=True, inferLong=True)"
						],
						"outputs": [],
						"execution_count": 6
					}
				]
			},
			"dependsOn": []
		}
	]
}